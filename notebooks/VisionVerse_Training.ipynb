{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# VisionVerse Training\n",
    "\n",
    "This notebook demonstrates how to train the image captioning and classification models in the VisionVerse project."
   ],
   "id": "ff88925f3803a24b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dcf55544d416182"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"import sys\",\n",
    "    \"sys.path.append('../')\",\n",
    "   \n",
    "    \"import torch\",\n",
    "    \"import torch.nn as nn\",\n",
    "    \"import torch.optim as optim\",\n",
    "    \"from torch.utils.data import DataLoader\",\n",
    "    \"from torchvision import transforms\",\n",
    "  \n",
    "    \"from src.captioning.model import CNNtoRNN\",\n",
    "    \"from src.classification.model import CNNModel\",\n",
    "    \"from src.utils.data_loader import ImageCaptionDataset, get_loader\",\n",
    "    \"from src.captioning.utils import save_checkpoint\",\n",
    "    \"from src.classification.utils import load_category_names\",\n",
    "    \"from config import *\""
   ],
   "id": "dbd7f3e72fe06087"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Image Captioning Training",
   "id": "83a60ecbdbe6965a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " \"# Load vocabulary and prepare data\",\n",
    "    \"vocab = load_vocabulary('data/vocab.json')\",\n",
    "    \"transform = transforms.Compose([\",\n",
    "    \"    transforms.Resize((224, 224)),\",\n",
    "    \"    transforms.ToTensor()\",\n",
    "    \"    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\",\n",
    "    \"])\",\n",
    "   \n",
    "    \"train_loader = get_loader(\",\n",
    "    \"    root_folder='data/images'\",\n",
    "    \"    annotation_file='data/captions.txt'\",\n",
    "    \"    transform=transform,\",\n",
    "    \"    batch_size=CAPTION_BATCH_SIZE\",\n",
    "    \")\",\n",
    "\n",
    "    \"# Initialize model, loss, and optimizer\",\n",
    "    \"model = CNNtoRNN(CAPTION_EMBED_SIZE, CAPTION_HIDDEN_SIZE, len(vocab), CAPTION_NUM_LAYERS).to(DEVICE)\",\n",
    "    \"criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the pad token\",\n",
    "    \"optimizer = optim.Adam(model.parameters(), lr=CAPTION_LEARNING_RATE)\",\n",
    " \n",
    "    \"# Training loop\",\n",
    "    \"for epoch in range(CAPTION_NUM_EPOCHS):\",\n",
    "    \"    for idx, (imgs, captions) in enumerate(train_loader):\",\n",
    "    \"        imgs = imgs.to(DEVICE)\",\n",
    "    \"        captions = captions.to(DEVICE)\",\n",
    " \n",
    "    \"        outputs = model(imgs, captions)\",\n",
    "    \"        loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\",\n",
    "\n",
    "    \"        optimizer.zero_grad()\",\n",
    "    \"        loss.backward()\",\n",
    "    \"        optimizer.step()\",\n",
    "\n",
    "    \"        if idx % 100 == 0:\",\n",
    "    \"            print(f\\\"Epoch [{epoch+1}/{CAPTION_NUM_EPOCHS}], Step [{idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\\\")\",\n",
    "\n",
    "    \"# Save the trained model\\n\",\n",
    "    \"save_checkpoint(model, optimizer, 'checkpoints/caption_model.pth')\""
   ],
   "id": "5a737f0bff200f60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Image Classification Training",
   "id": "22919d7731dabb2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"# Load categories and prepare data\",\n",
    "    \"categories = load_category_names('data/flower_labels.json')\",\n",
    "    \"transform = transforms.Compose([\",\n",
    "    \"    transforms.RandomResizedCrop(224),\",\n",
    "    \"    transforms.RandomHorizontalFlip(),\",\n",
    "    \"    transforms.ToTensor(),\",\n",
    "    \"    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\",\n",
    "    \"])\",\n",
    "  \n",
    "    \"train_dataset = datasets.ImageFolder(root='data/flowers/train', transform=transform)\",\n",
    "    \"train_loader = DataLoader(train_dataset, batch_size=CLASSIFICATION_BATCH_SIZE, shuffle=True)\",\n",
    "\n",
    "    \"# Initialize model, loss, and optimizer\",\n",
    "    \"model = CNNModel(CLASSIFICATION_ARCH, CLASSIFICATION_HIDDEN_UNITS, len(categories)).to(DEVICE)\",\n",
    "    \"criterion = nn.CrossEntropyLoss()\",\n",
    "    \"optimizer = optim.Adam(model.parameters(), lr=CLASSIFICATION_LEARNING_RATE)\",\n",
    " \n",
    "    \"# Training loop\",\n",
    "    \"for epoch in range(CLASSIFICATION_NUM_EPOCHS):\",\n",
    "    \"    for idx, (images, labels) in enumerate(train_loader):\",\n",
    "    \"        images = images.to(DEVICE)\",\n",
    "    \"        labels = labels.to(DEVICE)\",\n",
    "\n",
    "    \"        outputs = model(images)\",\n",
    "    \"        loss = criterion(outputs, labels)\",\n",
    "\n",
    "    \"        optimizer.zero_grad()\",\n",
    "    \"        loss.backward()\",\n",
    "    \"        optimizer.step()\",\n",
    " \n",
    "    \"        if idx % 100 == 0:\",\n",
    "    \"            print(f\\\"Epoch [{epoch+1}/{CLASSIFICATION_NUM_EPOCHS}], Step [{idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\\\")\",\n",
    "\n",
    "    \"# Save the trained model\",\n",
    "    \"torch.save(model.state_dict(), 'checkpoints/classification_model.pth')\""
   ],
   "id": "3acc89a86e5de5c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
